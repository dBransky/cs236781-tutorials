{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## $$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\ip}[3]{\\left<#1,#2\\right>_{#3}}\n",
    "\\newcommand{\\E}[2][]{\\mathbb{E}_{#1}\\left[#2\\right]}\n",
    "$$\n",
    "\n",
    "# CS236781: Deep Learning\n",
    "# Tutorial 5: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "- Descent-based optimization\n",
    "- Back-propagation\n",
    "- Automatic differentiation\n",
    "- PyTorch backward functions\n",
    "- Bi-level differentiable optimization\n",
    "- Time-series prediction with CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:36.285868Z",
     "iopub.status.busy": "2020-11-26T07:05:36.285102Z",
     "iopub.status.idle": "2020-11-26T07:05:37.410438Z",
     "shell.execute_reply": "2020-11-26T07:05:37.410973Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.414389Z",
     "iopub.status.busy": "2020-11-26T07:05:37.413786Z",
     "iopub.status.idle": "2020-11-26T07:05:37.437351Z",
     "shell.execute_reply": "2020-11-26T07:05:37.438120Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 14\n",
    "data_dir = os.path.expanduser('~/.pytorch-datasets')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Theory Reminders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Descent-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As we have seen, training deep neural network is performed iteratively using descent-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The general scheme is,\n",
    "\n",
    "1. Initialize parameters to some $\\vec{\\Theta}^0 \\in \\set{R}^P$, and set $k\\leftarrow 0$.\n",
    "2. While not converged:\n",
    "    1. Choose a direction $\\vec{d}^k\\in\\set{R}^P$\n",
    "    2. Choose a step size $\\eta_k\\in\\set{R}$\n",
    "    3. Update: $\\vec{\\Theta}^{k+1} \\leftarrow \\vec{\\Theta}^k + \\eta_k \\vec{d}^k$\n",
    "    4. $k\\leftarrow k+1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Which descent direction to choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The one which maximally decreases the loss function $L(\\vec{\\Theta})$:\n",
    "\n",
    "$$\n",
    "\\vec{d} =\\arg\\min_{\\vec{d'}} L(\\vec{\\Theta}+\\vec{d'})-L(\\vec{\\Theta})\n",
    "\\approx\n",
    "\\arg\\min_{\\vec{d'}}\\nabla L(\\vec{\\Theta})^\\top\\vec{d'}, \\\n",
    "\\mathrm{s.t.} \\norm{\\vec{d}}_p=1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Choice of norm determines $\\vec{d}$. For example,\n",
    "- $p=1$: Coordinate descent: direction of the largest gradient component.\n",
    "- $p=2$: Gradient descent: $\\vec{d}=-\\nabla L(\\vec{\\Theta})$.\n",
    "\n",
    "|$p=1$|$p=2$|\n",
    "|---|---|\n",
    "|<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e3/Coordinate_descent.svg\" width=\"300\" /> | <img src=\"https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg\" width=\"300\" />| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Drawbacks and mitigations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Susceptible to initialization**\n",
    "\n",
    "Initializing near local minima can prevent finding better ones.\n",
    "\n",
    "<center><img src=\"imgs/sgd-init.png\" width=\"600\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can use **stochastic** gradient descent to get a different loss surface every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Why is it called stochastic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In mini-batch **stochastic** gradient descent, only one or a few samples are considered at each optimization step, selected by random sampling (ideally iid from $\\mathcal{D}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So, the effective loss surface is stochastic!\n",
    "Intuitively, this aids in convergence to a better solution because it prevents getting stuck in bad local minimal.\n",
    "\n",
    "<center><img src=\"imgs/sgd-loss.png\" width=\"500\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sensitive to learning rate**\n",
    "\n",
    "<center><img src=\"imgs/sgd-lr.png\" width=\"800\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Line search (1D minimization):\n",
    "$$\n",
    "\\eta_k = \\arg\\min_{\\eta'} L(\\vec{\\Theta}^k+\\eta'\\vec{d}^k)\n",
    "$$\n",
    "\n",
    "- Adaptive LR optimizers, e.g. Adam\n",
    "\n",
    "- LR scheduling\n",
    "<center><img src=\"imgs/sgd-lr-schedule.png\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Zig-zags in narrow \"ravines\"**\n",
    "\n",
    "<center><img src=\"imgs/sgd-zigzag.png\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Momentum: Use previous gradients to build \"speed\" in the common direction and cancel-out oscillations in opposite directions.\n",
    "\n",
    "- BatchNorm: Normalizes activations to zero-mean and unit variance (reduces curvature)\n",
    "\n",
    "- Second-order methods: Use quadratic local approximation of the loss surface, instead of linear.\n",
    "    - Newton's method: $\\vec{d}_k=\\mat{H}_k^{-1}\\vec{g}_k = \\nabla^2 L(\\vec{\\Theta}_k)^{-1}\\nabla L(\\vec{\\Theta}_k)$.\n",
    "    - Quasi-Newton methods which use some estimate of the Hessian based on first-order information (e.g. BFGS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The back-propagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "All the above optimization methods have a crucial thing in common: They require calculation of gradients of the loss w.r.t. to the parameters.\n",
    "\n",
    "In practical settings when training neural networks we have many different parameters tensors we would like to update separately. Thus, we require the gradient of the loss w.r.t. each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Back-propagation is an efficient way to calculate these gradients using the chain rule.\n",
    "\n",
    "We represent the application of a model to its inputs as a **computation graph**: nodes represent variables and edges represent functions.\n",
    "\n",
    "For example, a simple linear regression model can be represented as:\n",
    "\n",
    "<center><img src=\"imgs/backprop-graph.png\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Imagine that in this graph we have $N$ variables $\\vec{v}^i,\\ 1\\leq i \\leq N$  and functions $f_i$ which compute them from other variables.\n",
    "\n",
    "The graph is directional, thus assume $\\vec{v}^1, \\vec{v}^2,\\dots,\\vec{v}^N$ represents a topological order of the graph (parents before children).\n",
    "\n",
    "Define also the notation $\\delta\\vec{v}\\triangleq \\pderiv{L}{\\vec{v}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The forward pass can therefore be written as:\n",
    "\n",
    "1. For $i=1,2,\\dots,N$:\n",
    "  1. Graph parents of current node: $$\\mathcal{P}_i \\leftarrow \\left\\{\\vec{v}^j ~\\middle\\vert~ \\vec{v}^j \\text{ parent of } \\vec{v}^i\\right\\}$$ \n",
    "  2. Evaluate function at current node: $$\\vec{v}^i\\leftarrow f_i(\\mathcal{P}_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And in the backward pass we traverse the graph in reverse and apply the chain rule:\n",
    "\n",
    "1. Set $\\delta\\vec{v}^N=1$.\n",
    "2. For $i=N,N-1,\\dots,1$:\n",
    "  1. Graph children of current node: $$\\mathcal{C}_i \\leftarrow \\left\\{\\vec{v}^j ~\\middle\\vert~ \\vec{v}^j \\text{ child of } \\vec{v}^i\\right\\}$$  \n",
    "  2. Chain rule: $$\\delta\\vec{v}^i\\leftarrow \\sum_{\\vec{v}^j\\in\\mathcal{C}_i} \\delta\\vec{v}^j\\pderiv{\\vec{v}^j}{\\vec{v}^i}$$\n",
    "  \n",
    "Notes:\n",
    "1. The expression $\\delta\\vec{v}^j\\pderiv{\\vec{v}^j}{\\vec{v}^i}$ is a \"vector\"-Jacobian product (VJP).\n",
    "2. When a computation node's output is used by more than one other node (more than one child in the graph), we sum the incoming gradients from these children. This again arises directly from the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Backpropagation easily lends itself to a modular and efficient implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Modularity:\n",
    "- Nodes in the computation graph only need to know how to calculate their own derivatives.\n",
    "- This is then passed to the parent nodes, which can do the same.\n",
    "\n",
    "<center><img src=\"imgs/backprop-modular.png\" width=\"700\"/></center>\n",
    "\n",
    "(in the figure, $\\bar{z}$ means $\\delta z$ in our notation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Efficiency:\n",
    "\n",
    "- Only need to compute each $\\delta\\vec{v}^i$ once.\n",
    "- No need to construct the Jacobian, instead calculate the VJP directly since that's what we actually need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Modern automatic-differentiation packages such as PyTorch's `autograd` utilize exactly these tricks to implement backprop in an extremely powerful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Custom automatic differentiation with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll now learn how to extend PyTorch's `autograd` by defining our own custom nodes in the computation graph.\n",
    "\n",
    "Lets first introduce a cousin of ReLU, the Exponential-Linear Unit (ELU) activation function:\n",
    "\n",
    "$$\n",
    "f(z) =\n",
    "\\begin{cases}\n",
    "z, & z > 0\\\\\n",
    "\\alpha \\left(e^{z}-1\\right) & z \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll pretend PyTorch does not include this activation function and implement a custom version ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.443662Z",
     "iopub.status.busy": "2020-11-26T07:05:37.443153Z",
     "iopub.status.idle": "2020-11-26T07:05:37.470617Z",
     "shell.execute_reply": "2020-11-26T07:05:37.471489Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torchviz\n",
    "\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First, we'll implement just the actual computation as a standalone function so that we can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.476064Z",
     "iopub.status.busy": "2020-11-26T07:05:37.475458Z",
     "iopub.status.idle": "2020-11-26T07:05:37.496204Z",
     "shell.execute_reply": "2020-11-26T07:05:37.496850Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def elu_forward(z: Tensor, alpha: float):\n",
    "    elu_positive = z\n",
    "    elu_negative = alpha * (torch.exp(z) - 1)\n",
    "    elu_output = torch.where(z>0, elu_positive, elu_negative)\n",
    "    return elu_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A quick visualization to see what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.502913Z",
     "iopub.status.busy": "2020-11-26T07:05:37.501989Z",
     "iopub.status.idle": "2020-11-26T07:05:37.665896Z",
     "shell.execute_reply": "2020-11-26T07:05:37.665339Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = torch.linspace(-5, 5, steps=1000)\n",
    "plt.plot(z.numpy(), torch.relu(z).numpy(), label='ReLU(z)', linewidth=5);\n",
    "plt.plot(z.numpy(), elu_forward(z, alpha=0.5).numpy(), label='ELU(z)', linewidth=2); plt.legend(); plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we'll wrap it as an `nn.Module` so that we can use it as a layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.670136Z",
     "iopub.status.busy": "2020-11-26T07:05:37.669611Z",
     "iopub.status.idle": "2020-11-26T07:05:37.691511Z",
     "shell.execute_reply": "2020-11-26T07:05:37.692063Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ELU(torch.nn.Module):\n",
    "    \"\"\" ELU Activation layer \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, z: Tensor):\n",
    "        return elu_forward(z, self.alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "And as usual, we can look at the resulting computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.695619Z",
     "iopub.status.busy": "2020-11-26T07:05:37.695127Z",
     "iopub.status.idle": "2020-11-26T07:05:37.773795Z",
     "shell.execute_reply": "2020-11-26T07:05:37.774322Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"142pt\" height=\"264pt\"\n",
       " viewBox=\"0.00 0.00 141.97 264.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-260 137.97,-260 137.97,4 -4,4\"/>\n",
       "<!-- 140514282888496 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140514282888496</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"103.44,-20 -0.15,-20 -0.15,0 103.44,0 103.44,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"51.65\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\">SWhereBackward</text>\n",
       "</g>\n",
       "<!-- 140514282888592 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140514282888592</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.65,-256 23.65,-256 23.65,-224 77.65,-224 77.65,-256\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.65\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\">z</text>\n",
       "<text text-anchor=\"middle\" x=\"50.65\" y=\"-230.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (6)</text>\n",
       "</g>\n",
       "<!-- 140514282888592&#45;&gt;140514282888496 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140514282888592&#45;&gt;140514282888496</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.95,-223.62C41.54,-213.62 37.52,-200.22 35.65,-188 33.48,-173.85 34.33,-57.71 34.65,-56 36.29,-46.97 39.66,-37.41 42.96,-29.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"46.28,-30.59 47.12,-20.03 39.87,-27.76 46.28,-30.59\"/>\n",
       "</g>\n",
       "<!-- 140514282889168 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140514282889168</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"128.62,-188 44.67,-188 44.67,-168 128.62,-168 128.62,-188\"/>\n",
       "<text text-anchor=\"middle\" x=\"86.65\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\">ExpBackward</text>\n",
       "</g>\n",
       "<!-- 140514282888592&#45;&gt;140514282889168 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140514282888592&#45;&gt;140514282889168</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M59.73,-223.86C64.63,-215.69 70.72,-205.55 75.86,-196.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.01,-198.53 81.15,-188.15 73.01,-194.93 79.01,-198.53\"/>\n",
       "</g>\n",
       "<!-- 140514282887776 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140514282887776</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"133.8,-76 43.5,-76 43.5,-56 133.8,-56 133.8,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"88.65\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n",
       "</g>\n",
       "<!-- 140514282887776&#45;&gt;140514282888496 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140514282887776&#45;&gt;140514282888496</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82.2,-55.59C77.11,-48.16 69.86,-37.58 63.69,-28.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.56,-26.57 58.02,-20.3 60.78,-30.53 66.56,-26.57\"/>\n",
       "</g>\n",
       "<!-- 140514282889024 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140514282889024</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"133.47,-132 43.83,-132 43.83,-112 133.47,-112 133.47,-132\"/>\n",
       "<text text-anchor=\"middle\" x=\"88.65\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\">SubBackward0</text>\n",
       "</g>\n",
       "<!-- 140514282889024&#45;&gt;140514282887776 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140514282889024&#45;&gt;140514282887776</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M88.65,-111.59C88.65,-104.7 88.65,-95.1 88.65,-86.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.15,-86.3 88.65,-76.3 85.15,-86.3 92.15,-86.3\"/>\n",
       "</g>\n",
       "<!-- 140514282889168&#45;&gt;140514282889024 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140514282889168&#45;&gt;140514282889024</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M86.99,-167.59C87.25,-160.7 87.61,-151.1 87.92,-142.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"91.43,-142.42 88.3,-132.3 84.43,-142.16 91.43,-142.42\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fcc07ead9a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elu = ELU(alpha=0.5)\n",
    "z = torch.tensor([-2., -1, 0, 1, 2, 3], requires_grad=True)\n",
    "torchviz.make_dot(elu(z), params=dict(z=z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can see that the computation graph accurately represents the various basic mathematical operations performed by our `elu_forward` function.\n",
    "\n",
    "But what if we want to define the entire ELU operarion as one node in the graph?\n",
    "This can be useful e.g. for performance reasons.\n",
    "\n",
    "But how can we accomplish this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The solution is to use a lower-level PyTorch API, `autograd.Function`\n",
    "which allows us to define a function in terms of both it's forwards pass\n",
    "(the regular output computation), and it's **backward** pass\n",
    "(the gradient w.r.t. all it's inputs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From the PyTorch docs:\n",
    "    \n",
    "    Every operation performed on Tensor s creates a new Function object, that performs the computation, and records that it happened. The history is retained in the form of a DAG of functions, with edges denoting data dependencies (input <- output). Then, when backward is called, the graph is processed in the topological ordering, by calling backward() methods of each Function object, and passing returned gradients on to next Function s.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll first calculate the simple analytic derivative of the ELU function:\n",
    "$$\n",
    "\\pderiv{f(z)}{z} = f'(z) = \n",
    "\\begin{cases}\n",
    "1, & z > 0\\\\\n",
    "\\alpha e^{z} & z \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we need to figure out how to compute the vector-Jacobian product efficiently.\n",
    "Note that for any **elementwise** operation, $\\vec{y}=f(\\vec{x}),\\ f:\\set{R}^n\\rightarrow\\set{R}^n$, we can write the Jacobian as\n",
    "\n",
    "$$\n",
    "\\pderiv{\\vec{y}}{\\vec{x}} = \\pmatrix{\n",
    "\\ddots & \\vdots & \\\\\n",
    "\\cdots & \\pderiv{y_i}{x_j} & \\cdots \\\\\n",
    "& \\vdots & \\ddots\\\\\n",
    "}\n",
    "=\n",
    "\\pmatrix{\n",
    "f'(x_1) &  &  \\\\\n",
    "  & f'(x_i) &  \\\\\n",
    "& & f'(x_n)\\\\\n",
    "}\n",
    "= \\diag\\{{f'(\\vec{x})}\\}\n",
    "$$\n",
    "\n",
    "And it follows that the VJP can be computed simply:\n",
    "$$\n",
    "\\delta \\vec{x} = \\delta{\\vec{y}}\\pderiv{\\vec{y}}{\\vec{x}} = \\delta{\\vec{y}} \\odot f'(\\vec{x}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, equipped with the expression for the VJP, we can proceed to implement the Function object representing ELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.779970Z",
     "iopub.status.busy": "2020-11-26T07:05:37.779435Z",
     "iopub.status.idle": "2020-11-26T07:05:37.802687Z",
     "shell.execute_reply": "2020-11-26T07:05:37.803153Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class ELUFunction(autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, z: Tensor, alpha: float):\n",
    "        elu = elu_forward(z, alpha) # Regular forward pass computation from before\n",
    "        ctx.save_for_backward(z)    # Tensors should be saved using this method\n",
    "        ctx.alpha = alpha           # other properties can be saved like so\n",
    "        return elu\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        z, = ctx.saved_tensors      # Validates that no in-place modifications happened on saved tensors\n",
    "        alpha = ctx.alpha\n",
    "        \n",
    "        # Calculate diagonal of d(elu(z))/dz\n",
    "        grad_positive = torch.ones_like(z)\n",
    "        grad_negative = alpha * torch.exp(z)\n",
    "        \n",
    "        # Note: This is not the full Jacobian, it's the diagonal\n",
    "        grad_elu = torch.where(z>0, grad_positive, grad_negative)\n",
    "        \n",
    "        # Gradient of the loss w.r.t. our output\n",
    "        δ_elu = grad_output\n",
    "        \n",
    "        # Calcualte δz = d(elu(z))/dz * δ_elu\n",
    "        # Note: elementwise multiplication equivalant to vector-Jacobian product\n",
    "        δz = grad_elu * δ_elu\n",
    "        return δz, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now use this custom `Function` either directly or as part of a layer.\n",
    "\n",
    "For example, here's an ELU layer using our custom backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.807962Z",
     "iopub.status.busy": "2020-11-26T07:05:37.807162Z",
     "iopub.status.idle": "2020-11-26T07:05:37.830365Z",
     "shell.execute_reply": "2020-11-26T07:05:37.830891Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ELUCustom(torch.nn.Module):\n",
    "    \"\"\" ELU Layer with a custom backward pass \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, z: Tensor):\n",
    "        # Function.apply() invokes the forward pass\n",
    "        return ELUFunction.apply(z, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.834198Z",
     "iopub.status.busy": "2020-11-26T07:05:37.833716Z",
     "iopub.status.idle": "2020-11-26T07:05:37.886596Z",
     "shell.execute_reply": "2020-11-26T07:05:37.887247Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"139pt\" height=\"96pt\"\n",
       " viewBox=\"0.00 0.00 138.64 96.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 92)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-92 134.64,-92 134.64,4 -4,4\"/>\n",
       "<!-- 140514282682464 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140514282682464</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"130.46,-20 0.18,-20 0.18,0 130.46,0 130.46,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"65.32\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\">ELUFunctionBackward</text>\n",
       "</g>\n",
       "<!-- 140514258467616 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140514258467616</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"56.32,-88 2.32,-88 2.32,-56 56.32,-56 56.32,-88\"/>\n",
       "<text text-anchor=\"middle\" x=\"29.32\" y=\"-74.4\" font-family=\"Times,serif\" font-size=\"12.00\">z</text>\n",
       "<text text-anchor=\"middle\" x=\"29.32\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (6)</text>\n",
       "</g>\n",
       "<!-- 140514258467616&#45;&gt;140514282682464 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140514258467616&#45;&gt;140514282682464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M38.4,-55.86C43.31,-47.69 49.39,-37.55 54.53,-28.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57.68,-30.53 59.83,-20.15 51.68,-26.93 57.68,-30.53\"/>\n",
       "</g>\n",
       "<!-- 140514282956032 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140514282956032</title>\n",
       "<polygon fill=\"orange\" stroke=\"black\" points=\"128.32,-82 74.32,-82 74.32,-62 128.32,-62 128.32,-82\"/>\n",
       "<text text-anchor=\"middle\" x=\"101.32\" y=\"-68.4\" font-family=\"Times,serif\" font-size=\"12.00\">(6)</text>\n",
       "</g>\n",
       "<!-- 140514282956032&#45;&gt;140514282682464 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140514282956032&#45;&gt;140514282682464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.85,-61.89C90.62,-53.17 82.58,-39.77 76.06,-28.91\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"78.98,-26.97 70.84,-20.2 72.98,-30.57 78.98,-26.97\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fcc06763f10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elu_custom = ELUCustom(alpha=0.5)\n",
    "z = torch.tensor([-2., -1, 0, 1, 2, 3], requires_grad=True)\n",
    "torchviz.make_dot(elu_custom(z), params=dict(z=z),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This only tested the forward pass. Let's now put our custom layer in the context of a larger model and see that we can backprop through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.892039Z",
     "iopub.status.busy": "2020-11-26T07:05:37.891527Z",
     "iopub.status.idle": "2020-11-26T07:05:37.923823Z",
     "shell.execute_reply": "2020-11-26T07:05:37.924349Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (1): ELUCustom()\n",
       "  (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (3): ELUCustom()\n",
       "  (4): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elu_mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=512, out_features=1024),\n",
    "    ELUCustom(alpha=0.01),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    ELUCustom(alpha=0.01),\n",
    "    torch.nn.Linear(in_features=1024, out_features=10),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "elu_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.927719Z",
     "iopub.status.busy": "2020-11-26T07:05:37.927211Z",
     "iopub.status.idle": "2020-11-26T07:05:37.988331Z",
     "shell.execute_reply": "2020-11-26T07:05:37.988871Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.42.3 (20191010.1750)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"441pt\" height=\"548pt\"\n",
       " viewBox=\"0.00 0.00 440.99 548.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 544)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-544 436.99,-544 436.99,4 -4,4\"/>\n",
       "<!-- 140514283052240 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140514283052240</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"326.97,-20 229.03,-20 229.03,0 326.97,0 326.97,-20\"/>\n",
       "<text text-anchor=\"middle\" x=\"278\" y=\"-6.4\" font-family=\"Times,serif\" font-size=\"12.00\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 140514283052384 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>140514283052384</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"330.81,-76 225.19,-76 225.19,-56 330.81,-56 330.81,-76\"/>\n",
       "<text text-anchor=\"middle\" x=\"278\" y=\"-62.4\" font-family=\"Times,serif\" font-size=\"12.00\">SoftmaxBackward</text>\n",
       "</g>\n",
       "<!-- 140514283052384&#45;&gt;140514283052240 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>140514283052384&#45;&gt;140514283052240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M278,-55.59C278,-48.7 278,-39.1 278,-30.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.5,-30.3 278,-20.3 274.5,-30.3 281.5,-30.3\"/>\n",
       "</g>\n",
       "<!-- 140514283052720 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>140514283052720</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"329.98,-132 226.02,-132 226.02,-112 329.98,-112 329.98,-132\"/>\n",
       "<text text-anchor=\"middle\" x=\"278\" y=\"-118.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140514283052720&#45;&gt;140514283052384 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>140514283052720&#45;&gt;140514283052384</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M278,-111.59C278,-104.7 278,-95.1 278,-86.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.5,-86.3 278,-76.3 274.5,-86.3 281.5,-86.3\"/>\n",
       "</g>\n",
       "<!-- 140514283052816 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>140514283052816</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"195,-200 141,-200 141,-168 195,-168 195,-200\"/>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-186.4\" font-family=\"Times,serif\" font-size=\"12.00\">4.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-174.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10)</text>\n",
       "</g>\n",
       "<!-- 140514283052816&#45;&gt;140514283052720 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>140514283052816&#45;&gt;140514283052720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M195.19,-168.17C212.69,-158.62 235.29,-146.3 252.45,-136.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"254.26,-139.93 261.37,-132.07 250.91,-133.79 254.26,-139.93\"/>\n",
       "</g>\n",
       "<!-- 140514282682048 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>140514282682048</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"343.14,-194 212.86,-194 212.86,-174 343.14,-174 343.14,-194\"/>\n",
       "<text text-anchor=\"middle\" x=\"278\" y=\"-180.4\" font-family=\"Times,serif\" font-size=\"12.00\">ELUFunctionBackward</text>\n",
       "</g>\n",
       "<!-- 140514282682048&#45;&gt;140514283052720 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>140514282682048&#45;&gt;140514283052720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M278,-173.89C278,-165.52 278,-152.84 278,-142.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.5,-142.2 278,-132.2 274.5,-142.2 281.5,-142.2\"/>\n",
       "</g>\n",
       "<!-- 140514283051088 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>140514283051088</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"243.98,-262 140.02,-262 140.02,-242 243.98,-242 243.98,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-248.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140514283051088&#45;&gt;140514282682048 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>140514283051088&#45;&gt;140514282682048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M203.96,-241.82C217.95,-231.09 241.33,-213.14 258.08,-200.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"260.34,-202.96 266.14,-194.1 256.08,-197.41 260.34,-202.96\"/>\n",
       "</g>\n",
       "<!-- 140514283051616 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>140514283051616</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"109,-336 55,-336 55,-304 109,-304 109,-336\"/>\n",
       "<text text-anchor=\"middle\" x=\"82\" y=\"-322.4\" font-family=\"Times,serif\" font-size=\"12.00\">2.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"82\" y=\"-310.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1024)</text>\n",
       "</g>\n",
       "<!-- 140514283051616&#45;&gt;140514283051088 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>140514283051616&#45;&gt;140514283051088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M107.24,-303.86C125.49,-292.9 150.01,-278.19 167.98,-267.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.94,-270.32 176.71,-262.17 166.34,-264.32 169.94,-270.32\"/>\n",
       "</g>\n",
       "<!-- 140514282682464 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>140514282682464</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"257.14,-330 126.86,-330 126.86,-310 257.14,-310 257.14,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"192\" y=\"-316.4\" font-family=\"Times,serif\" font-size=\"12.00\">ELUFunctionBackward</text>\n",
       "</g>\n",
       "<!-- 140514282682464&#45;&gt;140514283051088 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>140514282682464&#45;&gt;140514283051088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M192,-309.82C192,-300.17 192,-284.69 192,-272.32\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"195.5,-272.1 192,-262.1 188.5,-272.1 195.5,-272.1\"/>\n",
       "</g>\n",
       "<!-- 140514283052528 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>140514283052528</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154.98,-398 51.02,-398 51.02,-378 154.98,-378 154.98,-398\"/>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-384.4\" font-family=\"Times,serif\" font-size=\"12.00\">AddmmBackward</text>\n",
       "</g>\n",
       "<!-- 140514283052528&#45;&gt;140514282682464 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>140514283052528&#45;&gt;140514282682464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M115.38,-377.82C129.85,-367.09 154.05,-349.14 171.38,-336.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"173.78,-338.87 179.73,-330.1 169.61,-333.24 173.78,-338.87\"/>\n",
       "</g>\n",
       "<!-- 140514283050800 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>140514283050800</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"54,-472 0,-472 0,-440 54,-440 54,-472\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\">0.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-446.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1024)</text>\n",
       "</g>\n",
       "<!-- 140514283050800&#45;&gt;140514283052528 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>140514283050800&#45;&gt;140514283052528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.63,-439.69C56.6,-429.3 72.4,-415.57 84.54,-405.03\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"87.02,-407.52 92.27,-398.32 82.43,-402.23 87.02,-407.52\"/>\n",
       "</g>\n",
       "<!-- 140514283050320 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>140514283050320</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"134.49,-472 71.51,-472 71.51,-440 134.49,-440 134.49,-472\"/>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-458.4\" font-family=\"Times,serif\" font-size=\"12.00\">x</text>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-446.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10, 512)</text>\n",
       "</g>\n",
       "<!-- 140514283050320&#45;&gt;140514283052528 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>140514283050320&#45;&gt;140514283052528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M103,-439.69C103,-430.4 103,-418.44 103,-408.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"106.5,-408.32 103,-398.32 99.5,-408.32 106.5,-408.32\"/>\n",
       "</g>\n",
       "<!-- 140514283051280 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>140514283051280</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"223.97,-466 152.03,-466 152.03,-446 223.97,-446 223.97,-466\"/>\n",
       "<text text-anchor=\"middle\" x=\"188\" y=\"-452.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140514283051280&#45;&gt;140514283052528 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>140514283051280&#45;&gt;140514283052528</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M176.18,-445.82C162.35,-435.09 139.24,-417.14 122.69,-404.29\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"124.76,-401.47 114.72,-398.1 120.47,-407 124.76,-401.47\"/>\n",
       "</g>\n",
       "<!-- 140514283053008 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>140514283053008</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"225.49,-540 150.51,-540 150.51,-508 225.49,-508 225.49,-540\"/>\n",
       "<text text-anchor=\"middle\" x=\"188\" y=\"-526.4\" font-family=\"Times,serif\" font-size=\"12.00\">0.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"188\" y=\"-514.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1024, 512)</text>\n",
       "</g>\n",
       "<!-- 140514283053008&#45;&gt;140514283051280 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>140514283053008&#45;&gt;140514283051280</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M188,-507.69C188,-498.4 188,-486.44 188,-476.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"191.5,-476.32 188,-466.32 184.5,-476.32 191.5,-476.32\"/>\n",
       "</g>\n",
       "<!-- 140514282957120 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>140514282957120</title>\n",
       "<polygon fill=\"orange\" stroke=\"black\" points=\"238.99,-398 173.01,-398 173.01,-378 238.99,-378 238.99,-398\"/>\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-384.4\" font-family=\"Times,serif\" font-size=\"12.00\">(10, 1024)</text>\n",
       "</g>\n",
       "<!-- 140514282957120&#45;&gt;140514282682464 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>140514282957120&#45;&gt;140514282682464</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204.05,-377.82C201.98,-368.07 198.66,-352.37 196.02,-339.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"199.43,-339.16 193.93,-330.1 192.58,-340.61 199.43,-339.16\"/>\n",
       "</g>\n",
       "<!-- 140514283052192 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>140514283052192</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"346.97,-330 275.03,-330 275.03,-310 346.97,-310 346.97,-330\"/>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-316.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140514283052192&#45;&gt;140514283051088 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>140514283052192&#45;&gt;140514283051088</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M294.45,-309.82C274.47,-298.74 240.65,-279.98 217.37,-267.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"218.85,-263.89 208.41,-262.1 215.45,-270.01 218.85,-263.89\"/>\n",
       "</g>\n",
       "<!-- 140514283051568 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>140514283051568</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"351.49,-404 270.51,-404 270.51,-372 351.49,-372 351.49,-404\"/>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-390.4\" font-family=\"Times,serif\" font-size=\"12.00\">2.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"311\" y=\"-378.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (1024, 1024)</text>\n",
       "</g>\n",
       "<!-- 140514283051568&#45;&gt;140514283052192 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>140514283051568&#45;&gt;140514283052192</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M311,-371.69C311,-362.4 311,-350.44 311,-340.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"314.5,-340.32 311,-330.32 307.5,-340.32 314.5,-340.32\"/>\n",
       "</g>\n",
       "<!-- 140514282957568 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>140514282957568</title>\n",
       "<polygon fill=\"orange\" stroke=\"black\" points=\"327.99,-262 262.01,-262 262.01,-242 327.99,-242 327.99,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"295\" y=\"-248.4\" font-family=\"Times,serif\" font-size=\"12.00\">(10, 1024)</text>\n",
       "</g>\n",
       "<!-- 140514282957568&#45;&gt;140514282682048 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>140514282957568&#45;&gt;140514282682048</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M292.64,-241.82C290.12,-232.07 286.08,-216.37 282.88,-203.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"286.23,-202.91 280.34,-194.1 279.45,-204.66 286.23,-202.91\"/>\n",
       "</g>\n",
       "<!-- 140514283052864 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>140514283052864</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"432.97,-194 361.03,-194 361.03,-174 432.97,-174 432.97,-194\"/>\n",
       "<text text-anchor=\"middle\" x=\"397\" y=\"-180.4\" font-family=\"Times,serif\" font-size=\"12.00\">TBackward</text>\n",
       "</g>\n",
       "<!-- 140514283052864&#45;&gt;140514283052720 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>140514283052864&#45;&gt;140514283052720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M378.93,-173.89C359.24,-163.96 327.55,-147.98 304.93,-136.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"306.36,-133.38 295.85,-132 303.21,-139.63 306.36,-133.38\"/>\n",
       "</g>\n",
       "<!-- 140514283052960 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>140514283052960</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"431.49,-268 362.51,-268 362.51,-236 431.49,-236 431.49,-268\"/>\n",
       "<text text-anchor=\"middle\" x=\"397\" y=\"-254.4\" font-family=\"Times,serif\" font-size=\"12.00\">4.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"397\" y=\"-242.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (10, 1024)</text>\n",
       "</g>\n",
       "<!-- 140514283052960&#45;&gt;140514283052864 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>140514283052960&#45;&gt;140514283052864</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M397,-235.69C397,-226.4 397,-214.44 397,-204.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"400.5,-204.32 397,-194.32 393.5,-204.32 400.5,-204.32\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fcc07ed5dc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 512, requires_grad=True)\n",
    "torchviz.make_dot(elu_mlp(x).mean(), params=dict(list(elu_mlp.named_parameters()) + [('x', x)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's run the backward pass and make sure we have gradients on all parameter tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:37.993282Z",
     "iopub.status.busy": "2020-11-26T07:05:37.992592Z",
     "iopub.status.idle": "2020-11-26T07:05:38.028548Z",
     "shell.execute_reply": "2020-11-26T07:05:38.029164Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight 2.196277648636169e-07\n",
      "0.bias 8.581169552712709e-09\n",
      "2.weight 2.9875732820983103e-07\n",
      "2.bias 1.9374366999613812e-08\n",
      "4.weight 2.743185234521661e-07\n",
      "4.bias 3.837890716340553e-08\n"
     ]
    }
   ],
   "source": [
    "l = torch.sum(elu_mlp(x))\n",
    "l.backward()\n",
    "\n",
    "for name, param in elu_mlp.named_parameters():\n",
    "    print(f\"{name} {torch.norm(param.grad).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Differentiable Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now we'll tackle a more interesting use-case for defining our custom backward functions: differentiating though an inner (unconstrained) optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What if we want to solve an inner optimization problem as part of our model, while the parameters of the inner problem are also optimized by the end-to-end optimization of the entire model?\n",
    "\n",
    "<center><img src=\"imgs/bilevel.png\" width=\"600\"/></center>\n",
    "\n",
    "Training such a network end-to-end means we're trying to find:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{\\Theta}^\\ast\n",
    "&=\n",
    "\\arg\\min_{\\vec{\\Theta}} \\E[(\\vec{x},\\vec{y})\\sim D]{\\mathcal{L}(\\vec{y}, \\hat{\\vec{y}})}\\\\\n",
    "&=\n",
    "\\arg\\min_{\\vec{\\Theta}} \\E[(\\vec{x},\\vec{y})\\sim D]{\n",
    "\\mathcal{L}(\\vec{y}, \\arg\\min_{\\vec{y'}} f(\\vec{y'}, h_{\\vec{\\Theta}}(\\vec{x}) )\n",
    "}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This type of setting is also known as a bi-level optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "From the perspective of the inner problem, $\\vec{z}$ is a \"fixed\" parameter. \n",
    "\n",
    "However, from the perspective of end-to-end training, we're optimizing $\\vec{\\Theta}$ in order to reduce the final loss.\n",
    "\n",
    "Therefore, we can view this as learning to parameterize the inner task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What do we need in order to train such a model end-to-end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As usual, we must find a way to calculate a VJP, in this case:\n",
    "$$\n",
    "\\delta \\vec{z} = \\pderiv{\\hat{\\vec{y}}}{\\vec{z}}\\ \\delta\\hat{\\vec{y}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Assume that $\\vec{y}=\\arg\\min_{\\vec{y}'}f(\\vec{y}', \\vec{z})$.\n",
    "\n",
    "Since $\\vec{y}$ is a minimizer of the function $f$, the necessary optimality condition\n",
    "must hold: $$\\nabla_{y}f(y, z)=0.$$\n",
    "\n",
    "If we then perturb $\\vec{z}$ by $d\\vec{z}$, we'll get a slightly different minimizer, $\\vec{y}+d\\vec{y}$. Thus also,\n",
    "\n",
    "$$\\nabla_{y}f(y+dy, z+dz)=0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can take a first-order Taylor expansion of $\\nabla_{y}f$ around the point $(y, z)$:\n",
    "\n",
    "$$\n",
    "\\nabla_{y}f(y+dy, z+dz) \\approx \\nabla_{y}f(y, z) + \\nabla^{2}_{yy}f(y,z)dy + \\nabla^{2}_{yz}f(y, z)dz = 0.\n",
    "$$\n",
    "\n",
    "Since $\\nabla_{y}f(y, z)=0$, we rearrange to obtain:\n",
    "\n",
    "$$\n",
    "\\nabla^{2}_{yy}f(y,z)dy = -\\nabla^{2}_{yz}f(y, z)dz.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll denote the Hessians as $\\mat{K}=\\nabla^{2}_{yy}f(y,z)$ and $\\mat{R}=\\nabla^{2}_{yz}f(y, z)$. We then obtain,\n",
    "\n",
    "$$\n",
    "\\mat{K}d\\vec{y}=-\\mat{R}d\\vec{z}\\Longrightarrow d\\vec{y}=-\\mat{K}^{-1}\\mat{R}d\\vec{z}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The above equation means that we have found a linear relationship between the change in the function value, $d\\vec{y}$ and the change in the argument value, $d\\vec{z}$.\n",
    "This linear relationship must be, by definition, related to the gradient of $\\vec{y}$ w.r.t. $\\vec{z}$.\n",
    "\n",
    "Writing the above as an inner product, we have $$d\\vec{y}=\\ip{-\\mattr{R}\\mat{K^{-T}}}{d\\vec{z}}{}.$$\n",
    "\n",
    "Note that $\\mat{K}$ is a Hessian, therefore symmetric and we can drop the transpose.\n",
    "\n",
    "Finally, by the \"outer\" definition of the gradient we see that $$\\pderiv{\\vec{y}}{\\vec{z}}=-\\mattr{R}\\mat{K^{-1}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now back to our VJP: we need \n",
    "$$\n",
    "\\delta \\vec{z} =  \\pderiv{\\vec{y}}{\\vec{z}} \\delta\\vec{y} =\\mattr{R}\\mat{K^{-1}}\\delta\\vec{y}.\n",
    "$$\n",
    "\n",
    "We'll do the calculation in two steps:\n",
    "1. Calculate $\\delta\\vec{u}=\\mat{K}^{-1}\\delta\\vec{y}$: Equivalent to solving the linear system $\\mat{K}\\delta\\vec{u}=\\delta\\vec{y}$.\n",
    "2. Calculate $\\delta\\vec{z} = -\\mat{R}^\\top \\delta\\vec{u}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, based on this, we have a way to implement such an inner-optimization layer:\n",
    "\n",
    "**Forward pass**: Compute the optimal solution of the inner problem, either with a some solver or even a closed-form expression.\n",
    "\n",
    "**Backward pass**: Calculate $\\delta\\vec{z}$ using the two-step procedure described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that it's also possible to develop these expression for the constrained optimization case, by using the KKT conditions where we used the unconstrained optimality condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Before implementing the argmin layer, we need some required helpers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "First, let's implement a helper to calculate an approximated least-squares solution to a linear system of equations $\\mat{A}\\vec{x}=\\vec{b}$.\n",
    "The built-in `torch.solve()` only supports a square matrix A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:38.035484Z",
     "iopub.status.busy": "2020-11-26T07:05:38.034653Z",
     "iopub.status.idle": "2020-11-26T07:05:38.057939Z",
     "shell.execute_reply": "2020-11-26T07:05:38.058612Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def solve_ls(A: Tensor, b: Tensor, abs: float = 1e-6, rel: float = 1e-6) -> Tensor:\n",
    "    # Solves the system A x = b in a least-squares sense using SVD, and returns x\n",
    "    U, S, V = torch.svd(A)\n",
    "    th = max(rel * S[0].item(), abs)\n",
    "    # Clip singular values\n",
    "    Sinv = torch.where(S >= th, 1.0 / S, torch.zeros_like(S))\n",
    "    return V @ torch.diag(Sinv) @ (U.transpose(1, 0) @ b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quick test for solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:38.064170Z",
     "iopub.status.busy": "2020-11-26T07:05:38.063460Z",
     "iopub.status.idle": "2020-11-26T07:05:38.875178Z",
     "shell.execute_reply": "2020-11-26T07:05:38.875823Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "dtype = torch.float64\n",
    "torch.set_default_dtype(dtype)\n",
    "\n",
    "# Create a simple regression problem\n",
    "N, D = 1000, 20\n",
    "\n",
    "X, y, w_gt = make_regression(\n",
    "    n_samples=N, n_features=D, coef=True, random_state=42, bias=10, noise=1,\n",
    ")\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "X, y, w_gt = [ torch.from_numpy(t).to(dtype) for t in [X, y, w_gt] ]\n",
    "\n",
    "# Solve it and check the solution is close to the ground-truth\n",
    "w_hat = solve_ls(X, y)\n",
    "assert torch.allclose(w_hat, w_gt, rtol=0.1, atol=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Notice that in the formulation we used, we considered only one variables vector, $\\vec{y}$, one parameters vector, $\\vec{z}$, and moreover we treated them both as 1d vectors.\n",
    "\n",
    "In practice, however, when implementing deep neural networks, we deal with many different parameters tensors, virtually all of which high-dimensional.\n",
    "\n",
    "In this tutorial, we'll deal with the case of one variable tensor (of any shape) and any number of parameter tensors (of any shape). It's simple to use the approach here to extend to multiple variables as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:38.882020Z",
     "iopub.status.busy": "2020-11-26T07:05:38.881343Z",
     "iopub.status.idle": "2020-11-26T07:05:38.911875Z",
     "shell.execute_reply": "2020-11-26T07:05:38.912527Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(*z: Tensor):\n",
    "    # Flattens a sequence of tensors into one \"long\" tensor of shape (N,)\n",
    "    # Note: cat & reshape maintain differentiability!\n",
    "    flat_z = torch.cat([z_.reshape(-1) for z_ in z], dim=0)\n",
    "    return flat_z\n",
    "\n",
    "def unflatten_like(t_flat: Tensor, *z: Tensor):\n",
    "    # Un-flattens a \"long\" tensor into a sequence of multiple tensors of arbitrary shape\n",
    "    t_flat = t_flat.reshape(-1) # make sure it's 1d\n",
    "    ts = []\n",
    "    offset = 0\n",
    "    for z_ in z:\n",
    "        numel = z_.numel()\n",
    "        ts.append(\n",
    "            t_flat[offset:offset+numel].reshape_as(z_)\n",
    "        )\n",
    "        offset += numel\n",
    "    assert offset == t_flat.numel()\n",
    "    \n",
    "    return tuple(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quick test for `flatten`/`unflatten`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:38.916703Z",
     "iopub.status.busy": "2020-11-26T07:05:38.916085Z",
     "iopub.status.idle": "2020-11-26T07:05:38.948411Z",
     "shell.execute_reply": "2020-11-26T07:05:38.949044Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t1, t2 = torch.randn(3, 5), torch.randn(2, 4)\n",
    "t_flat = flatten(t1, t2)\n",
    "assert t_flat.shape == (t1.numel() + t2.numel(),)\n",
    "\n",
    "t1_, t2_ = unflatten_like(t_flat, t1, t2)\n",
    "assert torch.allclose(t1, t1_)\n",
    "assert torch.allclose(t2, t2_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, finally, we're equipped to write an `autograd.Function`\n",
    "which implements differentiable optimization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's start with the forward pass.\n",
    "Here we simply need to solve an unconstrained optimization problem specified in terms of an *objective function*, *variables* (just one supported) and *parameters*.\n",
    "\n",
    "We'll use the LBFGS algorithm (a quasi-Newton method as seen in the lectures) as a general purpose solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:38.956045Z",
     "iopub.status.busy": "2020-11-26T07:05:38.955196Z",
     "iopub.status.idle": "2020-11-26T07:05:38.989526Z",
     "shell.execute_reply": "2020-11-26T07:05:38.990298Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import LBFGS\n",
    "\n",
    "def argmin_forward(ctx, obj_fun, y, *z):\n",
    "    # Calculates forward pass though argmin layer: returns y_argmin\n",
    "    # obj_fun(y, *z) evaluates the optimization objective we want to minimize\n",
    "    \n",
    "    # Note: solving for y, treating the z's as constants\n",
    "    optimizer = LBFGS(params=(y,), lr=0.9, max_iter=1000)\n",
    "\n",
    "    # Closure for LBFGS which evaluates the loss and calcualtes\n",
    "    # gradients of the variables.\n",
    "    def _optimizer_step():\n",
    "        # zero gradients\n",
    "        y.grad = torch.zeros_like(y)\n",
    "            \n",
    "        # evaluate loss\n",
    "        f = obj_fun(y, *z)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        # Note: not calling backward() because we don't want to compute\n",
    "        # gradients for anything except y\n",
    "        δy = autograd.grad(f, (y,), create_graph=False)[0]\n",
    "        y.grad += δy\n",
    "        \n",
    "        return f\n",
    "\n",
    "    # Solve the optimization problem - will evaluate closure multiple times\n",
    "    f_min = optimizer.step(_optimizer_step,)\n",
    "    y_argmin = y # Note: y was modified in place\n",
    "\n",
    "    ctx.save_for_backward(y_argmin, *z)\n",
    "    ctx.obj_fun = obj_fun\n",
    "\n",
    "    return y_argmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now the backward pass.\n",
    "Here we use the two-step procedure shown above, to calculate $\\delta\\vec{z}$.\n",
    "\n",
    "We'll need to calculate the Hessians\n",
    "$\\mat{K}=\\nabla^{2}_{yy}f(y,z)$\n",
    "and $\\mat{R}=\\nabla^{2}_{yz}f(y, z)$,\n",
    "but luckily autograd can calculate this (by a second automatic differentiation).\n",
    "\n",
    "The only complication is the shapes of $\\vec{y}$ and $\\vec{z}$, but we'll flatten them with our helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:38.996220Z",
     "iopub.status.busy": "2020-11-26T07:05:38.995594Z",
     "iopub.status.idle": "2020-11-26T07:05:39.026408Z",
     "shell.execute_reply": "2020-11-26T07:05:39.027068Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.autograd.functional import hessian\n",
    "\n",
    "def argmin_backward(ctx, grad_output):\n",
    "    y_argmin, *z = ctx.saved_tensors\n",
    "    obj_fun = ctx.obj_fun\n",
    "    \n",
    "    flat_y = flatten(y_argmin)\n",
    "    flat_z = flatten(*z)\n",
    "    \n",
    "    # Wrap objective function so that we can call it with flat tensors\n",
    "    def obj_fun_flat(flat_y, flat_z):\n",
    "        y = unflatten_like(flat_y, y_argmin)\n",
    "        zs = unflatten_like(flat_z, *z)\n",
    "        return obj_fun(*y, *zs)\n",
    "    \n",
    "    # Compute the Hessians on flattened y and z\n",
    "    H = hessian(obj_fun_flat, inputs=(flat_y, flat_z), create_graph=False)\n",
    "    Hyy = K = H[0][0]\n",
    "    Hyz = R = H[0][1]\n",
    "\n",
    "    # Now we need to calculate δz = -R^T K^-1 δy\n",
    "    # 1. Solve system for δu: K δu = δy\n",
    "    δy = grad_output\n",
    "    δy = torch.reshape(δy, (-1, 1))\n",
    "    δu = solve_ls(K, δy) # solve_ls(A, B) solves A X = B\n",
    "\n",
    "    # 2. Calculate δz = -R^T δu\n",
    "    δz_flat = -R.transpose(0, 1) @ δu\n",
    "    \n",
    "    # Extract gradient of each individual z\n",
    "    δz = unflatten_like(δz_flat, *z)\n",
    "    δy = torch.reshape(δy, y_argmin.shape)\n",
    "    \n",
    "    return None, δy, *δz  # backward's outputs must correspond to forward's inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now wrap these functions in a `autograd.Function` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:39.031514Z",
     "iopub.status.busy": "2020-11-26T07:05:39.030757Z",
     "iopub.status.idle": "2020-11-26T07:05:39.063040Z",
     "shell.execute_reply": "2020-11-26T07:05:39.063625Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class ArgMinFunction(autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, obj_fun, y, *z):\n",
    "        return argmin_forward(ctx, obj_fun, y, *z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return argmin_backward(ctx, grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's run a quick test for `argmin_forward`: Can we solve the simple regression problem from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:39.068616Z",
     "iopub.status.busy": "2020-11-26T07:05:39.068050Z",
     "iopub.status.idle": "2020-11-26T07:05:39.109967Z",
     "shell.execute_reply": "2020-11-26T07:05:39.110677Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.2047e+00, -3.0670e-01, -4.1667e-01,  2.6047e-01, -4.7777e+00,\n",
       "        -6.6667e-02, -5.2230e+00, -1.6505e-03, -4.1456e-01,  8.1708e-02,\n",
       "        -1.9675e+00, -2.5322e+00,  3.7461e-02, -6.1826e-01, -5.8916e-02,\n",
       "        -1.4817e-01, -5.9335e-01, -4.2895e+00,  4.0791e-02, -9.4177e-07],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a simple linear regression objective with l1 and l2 regularization\n",
    "# (just to test more than one z)\n",
    "def obj_fun(w: Tensor, l1: Tensor, l2: Tensor):\n",
    "    loss = torch.mean((X @ w - y)**2)\n",
    "    reg1 = l1 * torch.mean(torch.abs(w))\n",
    "    reg2 = l2 * torch.mean(w ** 2)\n",
    "    return torch.sum(loss + reg1 + reg2)\n",
    "\n",
    "# Optimization variable - init to random noise\n",
    "w = torch.randn_like(w_gt)*0.1\n",
    "w.requires_grad = True\n",
    "\n",
    "# Optimization problem parameters\n",
    "l1 = torch.randn(1, 1, requires_grad=True)\n",
    "l2 = torch.randn(1, 1, requires_grad=True)\n",
    "\n",
    "# Function.apply() invokes the forward pass\n",
    "w_hat_argmin = ArgMinFunction.apply(obj_fun, w, l1, l2)\n",
    "\n",
    "assert torch.allclose(w_hat_argmin, w_gt, rtol=0.2, atol=3)\n",
    "w_gt-w_hat_argmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Quick test for `argmin_backward`: Do we get gradients on our $\\vec{z}$s ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:39.115411Z",
     "iopub.status.busy": "2020-11-26T07:05:39.114643Z",
     "iopub.status.idle": "2020-11-26T07:05:39.153908Z",
     "shell.execute_reply": "2020-11-26T07:05:39.154492Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=tensor(27.0424, grad_fn=<MeanBackward0>)\n",
      "\n",
      "w.grad=tensor([ 2.8594e-06, -3.6926e-07, -1.1004e-06,  1.6721e-06, -1.9148e-06,\n",
      "         3.0106e-07, -2.5167e-06,  3.1427e-05, -7.2007e-07,  2.1528e-06,\n",
      "         1.9412e-07, -3.2436e-07,  8.7729e-07,  4.7307e-07,  4.9107e-08,\n",
      "         1.9620e-06, -3.5911e-07, -6.1125e-07, -2.2783e-06,  4.8494e-04])\n",
      "l1.grad=None\n",
      "l2.grad=None\n",
      "\n",
      "w.grad=tensor([0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
      "        0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500, 0.0500,\n",
      "        0.0500, 0.0505])\n",
      "l1.grad=tensor([[0.]])\n",
      "l2.grad=tensor([[0.]])\n"
     ]
    }
   ],
   "source": [
    "loss = torch.mean(w_hat_argmin)\n",
    "print(f'{loss=}\\n')\n",
    "\n",
    "# Before backward: z (l1 & l2) gradients should be None\n",
    "print(f'{w.grad=}')\n",
    "print(f'{l1.grad=}')\n",
    "print(f'{l2.grad=}\\n')\n",
    "\n",
    "loss.backward()\n",
    "print(f'{w.grad=}')\n",
    "print(f'{l1.grad=}')\n",
    "print(f'{l2.grad=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's demonstrate how to use our `ArgMinFunction` in the context of a model.\n",
    "\n",
    "We'll tackle the task of **time-series prediction**, by applying linear regression on a learned, non-linear representation of the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll implement the following model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"imgs/enc-pred-dec.png\" width=\"1100\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Notes:\n",
    "\n",
    "1. The encoder and decoder will be simple 1D CNNs.\n",
    "2. The encoder calculates some non-linear embedding of the input while the decoder maps an embedding back to the input space.\n",
    "3. The predictor applies linear regression to try fit optimal weights for predict the last part of the embedding, $\\vec{Y}_e$ from the first part, $\\vec{X}_e$ (\"post-diction\"):\n",
    "    1. Postdiction: $\\vec{w}^\\ast=\\arg\\min_{\\vec{w}} \\norm{\\vec{X}_e\\vec{w}-\\vec{Y}_e}^2+\\lambda\\norm{\\vec{w}}^2$\n",
    "    2. Prediction: $\\hat{\\vec{Y}}_e=\\vec{Z}_e\\vec{w}^\\ast$ where $\\vec{Z}_e$ is the last part of the embedding with the same length as $\\vec{X}_e$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's load some data: We'll use a open dataset from Kaggle, containing 12 years of daily stock price data from equities included in the Dow Jones Industrial Average (DJIA).\n",
    "\n",
    "You can obtain this dataset [here](https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:39.158645Z",
     "iopub.status.busy": "2020-11-26T07:05:39.158005Z",
     "iopub.status.idle": "2020-11-26T07:05:39.680268Z",
     "shell.execute_reply": "2020-11-26T07:05:39.680831Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"DJIA_30/all_stocks_2006-01-01_to_2018-01-01.csv.gz\", compression='gzip')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:39.687419Z",
     "iopub.status.busy": "2020-11-26T07:05:39.686463Z",
     "iopub.status.idle": "2020-11-26T07:05:40.899674Z",
     "shell.execute_reply": "2020-11-26T07:05:40.899155Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot some stocks\n",
    "stock_names = [\"AAPL\", \"GOOGL\", \"MSFT\", \"AMZN\"]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "for stock_name in stock_names:\n",
    "    df_stock = df[df['Name'] == stock_name]\n",
    "    df_stock_dates = [datetime.strptime(d,'%Y-%m-%d').date() for d in df_stock['Date']]\n",
    "    ax.plot(df_stock_dates, df_stock['Close'], label=stock_name)\n",
    "ax.set_ylabel('Close Price ($)'); ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's do some minimal processing to make the data useable:\n",
    "\n",
    "1. Split the data into segments of a fixed number of days.\n",
    "2. Split each secment into a BASE ($\\vec{X}$) and a TARGET ($\\vec{Y}$).\n",
    "3. Split all segments into a training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:40.954365Z",
     "iopub.status.busy": "2020-11-26T07:05:40.953642Z",
     "iopub.status.idle": "2020-11-26T07:05:40.997565Z",
     "shell.execute_reply": "2020-11-26T07:05:40.998237Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "SEG_LEN = 40\n",
    "SEG_BASE = 30\n",
    "SEG_TARGET = SEG_LEN - SEG_BASE\n",
    "\n",
    "# Filter out only selected stocks\n",
    "df = df[df['Name'].isin(stock_names)]\n",
    "# Split into segments of SEG_LEN days\n",
    "X = torch.tensor(df['Close'].values, dtype=dtype)\n",
    "X = X[0:SEG_LEN*(X.shape[0]//SEG_LEN)]\n",
    "X = torch.reshape(X, (-1, 1, SEG_LEN)) # adding channel dimension\n",
    "\n",
    "# Train-test split\n",
    "test_ratio = 0.3\n",
    "N = X.shape[0]\n",
    "N_train = int(N * (1-test_ratio))\n",
    "idxs = torch.randperm(X.shape[0],)\n",
    "X_train, X_test = X[idxs[:N_train]], X[idxs[N_train:]]\n",
    "\n",
    "# Split out target segment at the end\n",
    "X_train, Y_train = X_train[..., :SEG_BASE], X_train[..., SEG_BASE:]\n",
    "X_test, Y_test = X_test[..., :SEG_BASE], X_test[..., SEG_BASE:]\n",
    "\n",
    "print(f\"{X_train.shape=}\\n{Y_train.shape=}\\n\\n\")\n",
    "print(f\"{X_test.shape=}\\n{Y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can plot some random BASE and TARGET pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.003268Z",
     "iopub.status.busy": "2020-11-26T07:05:41.002671Z",
     "iopub.status.idle": "2020-11-26T07:05:41.219474Z",
     "shell.execute_reply": "2020-11-26T07:05:41.220146Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "idx_perm = np.random.permutation(range(N_train))\n",
    "for i in range(10):\n",
    "    ax.plot(range(SEG_BASE), X_train[idx_perm[i], 0, :])\n",
    "    ax.plot(range(SEG_BASE, SEG_LEN), Y_train[idx_perm[i], 0, :])\n",
    "ax.axvline(x=SEG_BASE, color='k', linestyle=\":\", linewidth=\"5\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We'll expose the processed data to PyTorch `DalaLoader`s via the `TensorDataset` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.225198Z",
     "iopub.status.busy": "2020-11-26T07:05:41.224289Z",
     "iopub.status.idle": "2020-11-26T07:05:41.264708Z",
     "shell.execute_reply": "2020-11-26T07:05:41.265698Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "dl_train, dl_test = [\n",
    "    DataLoader(TensorDataset(X, Y), batch_size=BATCH_SIZE, shuffle=True)   \n",
    "    for X, Y in [(X_train, Y_train), (X_test, Y_test)]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both the encoder and decoder will use the same model, a 1D CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.276739Z",
     "iopub.status.busy": "2020-11-26T07:05:41.275305Z",
     "iopub.status.idle": "2020-11-26T07:05:41.342940Z",
     "shell.execute_reply": "2020-11-26T07:05:41.344182Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EncDec(torch.nn.Module):\n",
    "    def __init__(self, channels=[1, 4, 8], out_nl=True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        channel_pairs = zip(channels[:-1], channels[1:])\n",
    "        for in_channels, out_channels in channel_pairs:\n",
    "            layers.extend([\n",
    "                \n",
    "                torch.nn.Conv1d(\n",
    "                    in_channels, out_channels, kernel_size=5, padding=2, bias=True\n",
    "                ),\n",
    "                \n",
    "                ELUCustom(alpha=0.5),\n",
    "            ])\n",
    "        if not out_nl:\n",
    "            layers = layers[:-1]\n",
    "            \n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.350743Z",
     "iopub.status.busy": "2020-11-26T07:05:41.349720Z",
     "iopub.status.idle": "2020-11-26T07:05:41.408588Z",
     "shell.execute_reply": "2020-11-26T07:05:41.409271Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "enc = EncDec(channels=[1, 4, 8], out_nl=True)\n",
    "enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Test encoder forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.416454Z",
     "iopub.status.busy": "2020-11-26T07:05:41.415449Z",
     "iopub.status.idle": "2020-11-26T07:05:41.472640Z",
     "shell.execute_reply": "2020-11-26T07:05:41.473517Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x0, y0 = next(iter(dl_train))\n",
    "print(f\"{x0.shape=}\\n\")\n",
    "print(f\"{enc(x0).shape=}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now, our regression layer will use the custom `ArgMinFunction` to solve an optimization problem during the forward pass.\n",
    "\n",
    "Recall from the above definitions:\n",
    "\n",
    "- The predictor applies linear regression to try fit optimal weights for predict the last part of the embedding, $\\vec{Y}_e$ from the first part, $\\vec{X}_e$ (\"post-diction\"):\n",
    "    1. Postdiction: $\\vec{w}^\\ast=\\arg\\min_{\\vec{w}} \\norm{\\vec{X}_e\\vec{w}-\\vec{Y}_e}^2+\\lambda\\norm{\\vec{w}}^2$\n",
    "    2. Prediction: $\\hat{\\vec{Y}}_e=\\vec{Z}_e\\vec{w}^\\ast$ where $\\vec{Z}_e$ is the last part of the embedding with the same length as $\\vec{X}_e$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.481928Z",
     "iopub.status.busy": "2020-11-26T07:05:41.481207Z",
     "iopub.status.idle": "2020-11-26T07:05:41.529191Z",
     "shell.execute_reply": "2020-11-26T07:05:41.529843Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PredictorArgMinLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.prediction_len = in_features - out_features\n",
    "        self.prediction_target_len = out_features\n",
    "        \n",
    "        # We'll train both W and lambda\n",
    "        self.w = torch.nn.Parameter(torch.randn(\n",
    "            self.prediction_target_len, self.prediction_len, requires_grad=True,\n",
    "        ))\n",
    "        self.reg_lambda = torch.nn.Parameter(torch.tensor([1.], requires_grad=True))\n",
    "    \n",
    "    @staticmethod\n",
    "    def obj_fun(w: Tensor, x: Tensor, y: Tensor, reg_lambda: Tensor):\n",
    "        # Objective function performing linear regression\n",
    "        xw = torch.matmul(x, w.T)\n",
    "        loss = torch.mean((xw - y)**2)\n",
    "        reg = reg_lambda * torch.mean(w ** 2)\n",
    "        return torch.sum(loss + reg)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Postdiction\n",
    "        # X = | ------ X_e ------ | -- Y_e -- |\n",
    "        x_post = x[..., :self.prediction_len] # X_e\n",
    "        y_post = x[..., self.prediction_len:] # Y_e\n",
    "        w_opt = ArgMinFunction.apply(self.obj_fun, self.w, x_post, y_post, self.reg_lambda)\n",
    "        \n",
    "        # Prediction\n",
    "        # X = | --------- | ------ Z_e ------ |\n",
    "        x_pred = x[..., -self.prediction_len:] # Z_e in the text\n",
    "        \n",
    "        return torch.matmul(x_pred, w_opt.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we'll create a model containing the full architecture of encoder, predictor and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.537520Z",
     "iopub.status.busy": "2020-11-26T07:05:41.536722Z",
     "iopub.status.idle": "2020-11-26T07:05:41.572415Z",
     "shell.execute_reply": "2020-11-26T07:05:41.572960Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "class EncPredictorDec(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features: int, postdiction_length: int,\n",
    "        encoder_channels: List[int], decoder_channels: List[int]=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if decoder_channels is None:\n",
    "            decoder_channels = list(reversed(encoder_channels))\n",
    "            \n",
    "        self.enc = EncDec(encoder_channels, out_nl=True)\n",
    "        self.dec = EncDec(decoder_channels, out_nl=False)\n",
    "        self.pred = PredictorArgMinLayer(in_features, postdiction_length)\n",
    "        self.postdiction_length = postdiction_length\n",
    "        \n",
    "    def forward(self, x: Tensor):\n",
    "        # Calculate embeding\n",
    "        x_emb = self.enc(x)\n",
    "        \n",
    "        # Postdict then predict\n",
    "        y_hat_emb = self.pred(x_emb)\n",
    "        \n",
    "        # Decode back to input space\n",
    "        y_hat = self.dec(y_hat_emb)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Time to train! Notes:\n",
    "\n",
    "1. Here we define the \"outer\" optimizer which performs the end-to-end optimization.\n",
    "2. We'll demonstrate how to employ simple a simple decaying learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.577231Z",
     "iopub.status.busy": "2020-11-26T07:05:41.576719Z",
     "iopub.status.idle": "2020-11-26T07:05:41.616719Z",
     "shell.execute_reply": "2020-11-26T07:05:41.617250Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "model = EncPredictorDec(\n",
    "    in_features=SEG_BASE, postdiction_length=SEG_TARGET,\n",
    "    encoder_channels=[1, 4, 8, 16]\n",
    ")\n",
    "print(model)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# End-to-end optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=0.01, eps=1e-6)\n",
    "\n",
    "# Decay learning rate each epoch\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.622102Z",
     "iopub.status.busy": "2020-11-26T07:05:41.621504Z",
     "iopub.status.idle": "2020-11-26T07:05:41.657257Z",
     "shell.execute_reply": "2020-11-26T07:05:41.657960Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def run_epoch(model, dl, epoch_idx, max_batches, train=True):\n",
    "    desc = f'Epoch #{epoch_idx:02d}: {\"Training\" if train else \"Evaluating\"} '\n",
    "    losses = []\n",
    "    pbar = tqdm(dl, desc=desc)\n",
    "    for i, (x, y) in enumerate(pbar):\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y, y_pred)\n",
    "        \n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        pbar.desc = desc + f\"[loss={loss.item():.3f}]\"\n",
    "        if max_batches and i >= max_batches:\n",
    "            break\n",
    "    pbar.desc = desc + f\"avg. loss = {np.mean(losses)}\"\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T07:05:41.662181Z",
     "iopub.status.busy": "2020-11-26T07:05:41.661675Z",
     "iopub.status.idle": "2020-11-26T07:06:09.467643Z",
     "shell.execute_reply": "2020-11-26T07:06:09.468285Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 1\n",
    "max_batches = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    run_epoch(model, dl_train, epoch, max_batches, train=True)\n",
    "    with torch.no_grad():\n",
    "        run_epoch(model, dl_test, epoch, max_batches, train=False)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Credits**\n",
    "\n",
    "This tutorial was written by [Aviv A. Rosenberg](https://avivr.net).<br>\n",
    "To re-use, please provide attribution and link to the original.\n",
    "\n",
    "Some images in this tutorial were taken and/or adapted from the following sources:\n",
    "\n",
    "- Dr. Roger Grosse, UToronto, cs321\n",
    "- Fundamentals of Deep Learning, Nikhil Buduma, Oreilly 2017\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
